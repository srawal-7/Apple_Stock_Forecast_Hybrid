# -*- coding: utf-8 -*-
"""Proposal + EDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tqs_4F1jLTYCBW8yrqPmpIQ_dfw8qzaL

# Hybrid Stock Forecasting: Merging Market Signals with News Sentiment
DSCI 631 Final Project

Spring 2025

## Team Members:

**Likhita Nallapati** : ln433@drexel.edu

**Shriya Rawal** : sr3728@drexel.edu

Predicting future stock price movements is both a challenge and a necessity in today’s fast-paced financial markets. While traditional models rely heavily on historical price data, modern approaches increasingly leverage real-time financial news to gain a predictive edge.

In this project, we aim to forecast future stock price trends by combining technical indicators from historical stock prices with sentiment signals extracted from financial news headlines. By integrating structured numerical data with unstructured textual insights, our model captures both the quantitative and narrative forces influencing stock behavior.

Traditional models often miss critical sentiment-driven signals, particularly during periods of heightened market volatility. To address this gap, we will develop a system that merges technical and textual information to improve the accuracy of short-term stock price forecasts.

Specifically, we will:

- Predict future stock movements (up/down) and/or percentage changes based on historical and textual features.

- Compare the performance of our modeling strategies.

- Additionally, we will visualize and analyze the correlation between news sentiment spikes and stock price volatility to better understand market behavior.

## Dataset Overview
The data for this project will be sourced from two primary platforms. News headlines will be collected from Kaggle's Apple dataset: Historical Financial News Data. We will focus on Apple for this project, but our approach is designed to be adaptable to other companies and stocks. The dataset can be accessed here: https://www.kaggle.com/datasets/frankossai/apple-stock-aapl-historical-financial-news-data/data.

The dataset includes the following features:

* Date: The timestamp of the news article in ISO 8601 format.
* Title: The headline of the news article.
* Content: The body text of the article.
* Link: The URL link to the full news article.
* Symbols: Stock symbols related to the content (e.g., AAPL, MSFT).
* Tags: Additional keywords or categories assigned to the article.
* Sentiment Analysis:
  * Sentiment Polarity: An overall measure of sentiment in the article, ranging from negative to positive.
  * Sentiment Negative: The proportion of negative sentiment in the text.
  * Sentiment Neutral: The proportion of neutral sentiment in the text.
  * Sentiment Positive: The proportion of positive sentiment in the text.

Additionally, we used historical stock data for Apple, accessed through Yahoo Finance via the yfinance package. This will include features such as:

* Open: Opening price of the stock

* High: Highest price during the day

* Low: Lowest price during the day

* Close: Closing price of the stock

* Volume: Trading volume

Together, these datasets provide both the structured (price history) and unstructured (news sentiment) information necessary to build and evaluate hybrid forecasting models.

### Data Selection and Justification

For the historical stock data obtained via yfinance, we have retained the following features: Open, High, Low, Close, and Volume, as they provide essential technical indicators for modeling stock price movements. We dropped Dividends and Stock Splits, as these events occur infrequently and may be noise to the predictive models.

For the financial news dataset, we made the following selection decisions:

* We excluded the Tags feature because approximately 40% of its values were missing, making it unreliable for analysis without significant imputation or assumptions.

* We excluded the links and the provided sentiment scores (Sentiment Polarity, Sentiment Positive/Negative/Neutral) because our project aims to perform custom sentiment analysis using the FinBERT model. This allows us to maintain consistency in sentiment evaluation and avoid relying on pre-calculated scores of potentially varying quality.

By selecting these features, we ensure that:

* The technical data remains comprehensive for deriving indicators like moving averages or volatility measures.

* The news data remains flexible for applying a consistent, state-of-the-art sentiment analysis pipeline, enhancing the quality of the textual signals feeding into our models.

### Exhibition of Datasets
"""

# Libraries
import yfinance as yf
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import nltk
import string
import re
from datetime import datetime
from collections import Counter
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize

# yFinance Apple Historical Data

ticker_symbol = "AAPL"     # ticker symbol

ticker = yf.Ticker(ticker_symbol)   # ticker object

# Fetching historical data from 2016-01-01 to 2024-12-31
historical_data = ticker.history(start="2016-01-01", end="2024-12-31")

# Save historical data to CSV
historical_data[['Open', 'High', 'Low', 'Close', 'Volume']].to_csv('aapl_historical_data.csv')

print(historical_data[['Open', 'High', 'Low', 'Close', 'Volume']])

# Apple Daily News Dataset

import pandas as pd
news_df = pd.read_csv("/content/apple_news_data.csv")
print(news_df.head())

news_df.shape

"""### Data Preprocessing

To prepare the data for modeling, we performed initial cleaning by removing null values, dropping unused columns (e.g., tags, headline link, and pre-calculated sentiment scores), merge datasets, correcting date formats, and standardizing the text by lowercasing all news content. This ensures consistency and high-quality input for both time-series and NLP-based modeling steps.
"""

# looking at all features
print("News Data: " + str(news_df.columns.tolist()))
print("YFinance Data: " + str(historical_data.columns.tolist()))

print(news_df.info())
print('\n')
print(historical_data.info())

# checking to see null values per column
print("News data: ")
news_df.isna().sum()

# checking to see null values per column
print("yfinance data: ")
historical_data.isna().sum()

# dropping sentiment analysis and tags (40% values are null)
news_df = news_df.drop(['sentiment_polarity', 'sentiment_neg', 'sentiment_neu', 'sentiment_pos', 'tags','link'], axis = 1)

# formatting date to remove the additional timestamp in news_df
news_df['date'] = pd.to_datetime(news_df['date'], utc=True).dt.strftime('%m-%d-%Y')

# formatting the date column in historical data
historical_data = historical_data.reset_index()
historical_data = historical_data.rename(columns={'index': 'date'})
print(historical_data.columns)
print(historical_data.head())

historical_data = historical_data.rename(columns={'Date': 'date'})
historical_data = historical_data.drop(columns=['Dividends', 'Stock Splits'])

news_df['date'] = pd.to_datetime(news_df['date'])
historical_data['date'] = pd.to_datetime(historical_data['date'])

news_df['date'] = news_df['date'].dt.tz_localize(None)
historical_data['date'] = historical_data['date'].dt.tz_localize(None)

#merge the two datasets
merged_df = pd.merge(news_df, historical_data, on='date', how='inner')
print(merged_df.shape)
print(merged_df.head())

#save csv
merged_df.to_csv('merged_data.csv', index=False)

print("Merged Data: " + str(merged_df.columns.tolist()))

# text preprocessing
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')

# Lowercase text columns
merged_df['title'] = merged_df['title'].str.lower()
merged_df['content'] = merged_df['content'].str.lower()

# Tokenization
merged_df['tokenized_title'] = merged_df['title'].apply(word_tokenize)
merged_df['tokenized_content'] = merged_df['content'].apply(word_tokenize)

# Stop word removal
stop_words = set(stopwords.words("english"))
merged_df['tokenized_title'] = merged_df['tokenized_title'].apply(lambda tokens: [word for word in tokens if word not in stop_words])
merged_df['tokenized_content'] = merged_df['tokenized_content'].apply(lambda tokens: [word for word in tokens if word not in stop_words])

# Punctuation removal
merged_df['tokenized_title'] = merged_df['tokenized_title'].apply(lambda tokens: [word for word in tokens if word.isalnum()])
merged_df['tokenized_content'] = merged_df['tokenized_content'].apply(lambda tokens: [word for word in tokens if word.isalnum()])

# Removing numbers
merged_df['tokenized_title'] = merged_df['tokenized_title'].apply(lambda tokens: [word for word in tokens if not word.isdigit()])
merged_df['tokenized_content'] = merged_df['tokenized_content'].apply(lambda tokens: [word for word in tokens if not word.isdigit()])

# Lemmatization (keeping it separate to see the differne between lemmatized and non-lemmatized words)
lemmatizer = WordNetLemmatizer()
merged_df['tokenized_title'] = merged_df['tokenized_title'].apply(lambda tokens: [lemmatizer.lemmatize(word) for word in tokens])
merged_df['tokenized_content'] = merged_df['tokenized_content'].apply(lambda tokens: [lemmatizer.lemmatize(word) for word in tokens])

# looking at the top 5 rows of news_df
print(merged_df.head(5))

"""#### Feature Engineering

We plan to incorporate sentiment analysis to enrich the dataset with additional predictive signals. Specifically, we intend to use FinBERT, a transformer-based model fine-tuned for financial sentiment analysis, to extract sentiment scores from financial news articles, reports, or relevant textual data. These sentiment scores — representing positive, neutral, or negative market sentiment

### EDA

##### Stock Data Analysis

1. `Price Statistics`:
To get a quick overview of the stock prices (Open, High, Low, Close), we calculate key summary statistics (mean, median, min, max, std). This helps understand the typical range and volatility of the prices.

2. `Price Change (Close - Open) Distribution`:
By plotting the distribution of daily price changes (Close - Open), we can check if stock movements are normally distributed, skewed, or have heavy tails — important for modeling returns and risk.

  The distribution of daily price changes for Apple stock is roughly centered around zero, indicating that small gains and losses are most common. However, there is a slight positive skew, suggesting that Apple more frequently closes slightly higher than it opens.

3. `Volume Distribution (Log-Scaled)`:
Volume data is often heavily right-skewed, meaning a few days have extremely high trading. Applying a log transformation helps normalize the distribution for better visualization and later modeling.

  The graph shows that there is a a peak around log(1 + volume) 18. This suggests that most trading volumes cluster around that value. The log transformation was effective in reducing skewness and making the distribution more symmetrical, which is important for modeling tasks like regression.
"""

# Price Statistics
price_stats = merged_df[['Open', 'High', 'Low', 'Close']].agg(['mean', 'median', 'min', 'max', 'std'])
print("Price Statistics:\n", price_stats)

# Price Change
# Create a new column for price change
merged_df['Price_Change'] = merged_df['Close'] - merged_df['Open']

# Plot the distribution
plt.figure(figsize=(8, 6))
sns.histplot(merged_df['Price_Change'], bins=50, kde=True, color='skyblue')
plt.title('Price Change (Close - Open) Distribution')
plt.xlabel('Price Change')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

# Volume distribution (log scale)
# Check skewness
volume_skewness = merged_df['Volume'].skew()

# Plot volume distribution
plt.figure(figsize=(8, 6))
if abs(volume_skewness) > 1:
    sns.histplot(np.log1p(merged_df['Volume']), bins=50, kde=True, color='salmon')
    plt.title('Volume Distribution (Log-Scaled)')
    plt.xlabel('Log(1 + Volume)')
else:
    sns.histplot(merged_df['Volume'], bins=50, kde=True, color='salmon')
    plt.title('Volume Distribution')
    plt.xlabel('Volume')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

"""##### Time Series Analysis
1. `Close Price Over Time`: Plotting the Close price over time helps detect overall market trends, cycles, or unusual price jumps, providing intuition on the stock's behavior historically.

  The graph shows that there is a clear upward trend over time, meaning Apple's stock generally gained value historically. The short-term fluctuations (ups and downs) are visible, often around key events like product launches or earnings announcements.

2. `Volume Over Time`: Volume over time shows the intensity of trading activity, which often correlates with important events, news releases, or price volatility spikes

    Here, spikes in volume could potential indicate events. Higher trading volume often signals high investor interest, leading to greater price volatility (larger up or down moves).Stable or low volume periods suggest less market excitement and smaller price changes.
"""

# Close Price Over Time

merged_df['date'] = pd.to_datetime(merged_df['date'])

# Sort by date
df_sorted = merged_df.sort_values('date')

# Plot Close price over time
plt.figure(figsize=(12, 6))
sns.lineplot(x='date', y='Close', data=df_sorted, color='green')
plt.title('Close Price Over Time')
plt.xlabel('Date')
plt.ylabel('Close Price')
plt.grid(True)
plt.show()

# Plot Volume over time
plt.figure(figsize=(12, 6))
sns.lineplot(x='date', y='Volume', data=df_sorted, color='purple')
plt.title('Volume Over Time')
plt.xlabel('Date')
plt.ylabel('Volume')
plt.grid(True)
plt.show()

"""##### Text Analysis

To better understand the underlying themes and focus areas within the financial news articles, we performed text analysis on both the titles and the content. By visualizing the most frequent words, we can learn more about key entities, topics, and sentiment drivers that influence stock price movements.

From the analysis, it can be seen that terms like "apple" "stock" "buy" "nasdaq" "hedge fund" and "dow jones" appeared very frequently. This suggests a focus on major tech companies, market indices (NASDAQ, Dow Jones), and investment actions(for instance, buy recommendations, hedge fund activity). These shows that investor sentiment and news coverage are closely tied to stocks and market trends.
"""

# subplots to show graphs next to each other
fig, axes = plt.subplots(1,2, figsize = (15,5))

# Word cloud for Title
from wordcloud import WordCloud

text_title = ' '.join([word for tokens in merged_df['tokenized_title'] for word in tokens if word not in stop_words])
wordcloud = WordCloud(width = 800, height = 500, background_color = 'white').generate(text_title)
axes[0].imshow(wordcloud, interpolation='bilinear')
axes[0].set_axis_off()
axes[0].set_title('Most Frequet Words in Title')

# Word clousd for Content
from wordcloud import WordCloud

text_content = ' '.join([word for tokens in merged_df['tokenized_content'] for word in tokens if word not in stop_words])
wordcloud = WordCloud(width = 800, height = 500, background_color = 'white').generate(text_content)
axes[1].imshow(wordcloud, interpolation='bilinear')
axes[1].set_axis_off()
axes[1].set_title('Most Frequent Words in Content')

plt.show()

# Calculating word frequencies
word_counts_title = Counter([word for tokens in news_df['tokenized_title'] for word in tokens]).most_common(20)
words_title = [word for word, count in word_counts_title]
freq_title = [count for word, count in word_counts_title]

word_counts_content = Counter([word for tokens in news_df['tokenized_content'] for word in tokens]).most_common(20)
words_content = [word for word, count in word_counts_content] # Fixed: Using word_counts_content instead of word_counts_title
freq_content = [count for word, count in word_counts_content] # Fixed: Using word_counts_content instead of word_counts_title


fig, axes = plt.subplots(1,2, figsize = (15,5)) # Fixed: Changed size to axes

# Most frequent words in Title
axes[0].bar(words_title,freq_title)
axes[0].set_title('Top 20 Most Frequent Words in Title')
axes[0].set_xlabel('Words')
axes[0].set_ylabel('Frequency')
axes[0].set_xticks(range(len(words_title))) # Fixed: Providing tick positions
axes[0].set_xticklabels(words_title, rotation=90) # Fixed: Setting tick labels with rotation


# Most frequent words in Content
axes[1].bar(words_content,freq_content) # Fixed: Removed ax = [1]
axes[1].set_title('Top 20 Most Frequent Words in Content')
axes[1].set_xlabel('Words')
axes[1].set_ylabel('Frequency')
axes[1].set_xticks(range(len(words_content))) # Fixed: Providing tick positions
axes[1].set_xticklabels(words_content, rotation=90) # Fixed: Setting tick labels with rotation

plt.show()

"""##### Symbol Exploration

To better understand the market relationships highlighted in the news, we analyzed stocks frequently mentioned alongside AAPL; the visualization reveals key companies and tickers that often share coverage with Apple, suggesting thematic or sector-based connections influencing investor attention.

Many top co-occurring symbols like AAPL34.SA, AAPL.MX, APC.F, and APC.XETRA are international listings or variants of Apple on different stock exchanges (e.g., Brazil, Mexico, Germany) stating that news articles mentioning AAPL often reference its global presence across markets. Other symbols like AMZN.US (Amazon) and GOOGL.US (Google) show up frequently. News coverage may bundle these companies together in discussions like technology trends, market leadership, earnings seasons, etc.
"""

# Symbol Exploration
merged_df['symbols'].value_counts()

# Most common co-occurring symbols with AAPL

import re
from collections import Counter

co_occurrences = Counter()

# Filter rows where 'symbols' contains 'AAPL'
aapl_data = merged_df[merged_df['symbols'].str.contains('AAPL', na=False)]

for symbol_list in aapl_data['symbols']:
    # Extract symbols
    symbols = re.findall(r'[A-Z\d.]+', symbol_list)

    # Remove AAPL versions
    symbols = [s for s in symbols if s not in ('AAPL', 'AAPL.US')]

    # Update counter
    co_occurrences.update(symbols)

# Top 10 most common
most_common_symbols = co_occurrences.most_common(10)
print(most_common_symbols)

# Unpack most_common_symbols into two lists
symbols = [symbol for symbol, _ in most_common_symbols]
counts = [count for _, count in most_common_symbols]

# Plot
plt.figure(figsize=(10,6))
plt.barh(symbols, counts, color='skyblue')
plt.xlabel('Co-occurring Symbols')
plt.ylabel('Count')
plt.title('Top 10 Co-occurring Symbols with AAPL')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""### Potential Algorithms

For this project, we intend to apply a classification task, where the goal is to predict the direction of the stock price movement (e.g., whether the price will go up or down) rather than predicting the exact future price.

We will first set a baseline using Logistic Regression. This will provide an effective model for classification problems. To capture more complex relationships and feature interactions, we will use Random Forest. An ensemble method that helps reduce overfitting and improves generalization. Additionally, we will incorporate XGBoost, an optimized gradient boosting algorithm that has a strong performance in structured data tasks. Since stock price movements are naturally sequential and time-dependent, we also plan to explore deep learning models such as LSTM or GRU, which are well-suited for capturing long-term dependencies and patterns in time-series data Furthermore, we plan to integrate sentiment analysis at a later stage by analyzing news articles or financial social media posts.

This diverse selection of models will allow us to compare traditional machine learning approaches with advanced deep learning techniques, ensuring we can select the most effective strategy during Phase 2 of the project.

### Limitations

1. `Data Noise`: Stock price movements are highly influenced by unpredictable external factors (e.g., global news, policy changes) that may not be captured in historical price data alone.

2. `Limited Features`: Initially relying only on technical indicators (Open, High, Low, Close, Volume) might miss out on other important factors like macroeconomic data, earnings reports, or insider trading activities.

3. `Overfitting Risk`: Complex models like Random Forest, XGBoost, and LSTMs can easily overfit if not properly tuned, especially with limited or noisy data.
"""